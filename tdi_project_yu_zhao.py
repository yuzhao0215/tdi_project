# -*- coding: utf-8 -*-
"""TDI_project_Yu_Zhao.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T5Lva_uJzmTSj8b4c8TdB5bsRZXFIZo9

# TDI Project - Novel View of Buildings Using Neural Volume Rendering of Pictures
Yu Zhao  
yuzha0215@gmail.com


## Implementation

First load pytorch3D and import modules
"""

import os
import sys
import torch
need_pytorch3d=False
try:
    import pytorch3d
except ModuleNotFoundError:
    need_pytorch3d=True
if need_pytorch3d:
    if torch.__version__.startswith("1.10.") and sys.platform.startswith("linux"):
        # We try to install PyTorch3D via a released wheel.
        pyt_version_str=torch.__version__.split("+")[0].replace(".", "")
        version_str="".join([
            f"py3{sys.version_info.minor}_cu",
            torch.version.cuda.replace(".",""),
            f"_pyt{pyt_version_str}"
        ])
        !pip install fvcore iopath
        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html
    else:
        # We try to install PyTorch3D from source.
        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz
        !tar xzf 1.10.0.tar.gz
        os.environ["CUB_HOME"] = os.getcwd() + "/cub-1.10.0"
        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'

import torch
import numpy as np
from pytorch3d.structures import Volumes
from pytorch3d.renderer import (
    FoVPerspectiveCameras,
    VolumeRenderer,
    NDCGridRaysampler,
    EmissionAbsorptionRaymarcher,
    look_at_view_transform
)
import glob
from PIL import Image
from pytorch3d.transforms import so3_exp_map
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm


# obtain the utilized device
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    torch.cuda.set_device(device)
else:
    device = torch.device("cpu")

from google.colab import drive
drive.mount('/content/drive')

root_dir = '/content/drive/MyDrive/notredame/'
# root_dir = '/content/drive/MyDrive/toys/'

img_dir = root_dir + "images/"
filter = "*.jpg"

img_names = glob.glob(img_dir+filter)
img_names = sorted(img_names)
print(img_names)
images = []

resize_coeff = 0.5

img_width = -1
img_height = -1

for img_name in img_names:
    img = Image.open(img_name)
    width, height = img.size
    new_width = int(width * resize_coeff)
    new_height = int(height * resize_coeff)

    if img_width < 0:
      img_width = new_width

    if img_height < 0:
      img_height = new_height

    img = img.resize((new_width, new_height))
    img = (np.array(img) / 255.0)
    images.append(img)

print("Image Width is: {}, image height is: {}".format(img_width, img_height))
img_array = np.array(images)
img_tensor = torch.tensor(img_array).float()

# load notredame pointcloud
pointcloud = np.loadtxt(root_dir + '/pointclouds/pointclouds.txt')
pc_xyz = pointcloud[:, :3]
pc_rgb = pointcloud[:, 3:]
pc_center = np.mean(pc_xyz, axis=0)
print("pc_center {}".format(pc_center))

scale = np.max(np.linalg.norm(pc_xyz - pc_center.reshape(1, 3), axis=1))

# move pointcloud center to (0, 0, 0)
pc_xyz = pc_xyz - pc_center.reshape(1, 3)

# scale pc_xyz so that the bounding box size is 1
pc_xyz = pc_xyz / scale
print(np.max(pc_xyz, axis=0))
print(np.min(pc_xyz, axis=0))

print("Scale is: {}".format(scale))

# load camera parameters
camera_parameters_foler = root_dir + 'camera_parameters/'
Rs = np.loadtxt(camera_parameters_foler + "Rs.txt").reshape((-1, 3, 3))
Ts = np.loadtxt(camera_parameters_foler + "Ts.txt").reshape(-1, 3)
# Ks = np.loadtxt(camera_parameters_foler + "Ks.txt").reshape((-1, 4, 4))

focal_length = 3036 * (img_width / 4032)  # in pixels
fov = 2 * np.arctan(img_width * 0.5 / focal_length) * 180 / np.pi
znear = 0.1
print("fov is: {}".format(fov))

# move cameras
num_cameras = Ts.shape[0]
for i in range(num_cameras):
  R = Rs[i].T
  T = -np.dot(R, Ts[i])
  Rs[i] = R
  Ts[i] = (T + pc_center) / scale
  print("camera {}th position: {}".format(i, Ts[i]))


R = torch.tensor(Rs, device=device)
T = torch.tensor(Ts, device=device)

cameras = FoVPerspectiveCameras(device=device, R=R, T=T, fov=fov, znear=znear, degrees=True)

from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
from pytorch3d.vis.plotly_vis import get_camera_wireframe
import plotly.graph_objects as go


def plot_scene(cameras, pc_xyz, pc_rgb, s=4):
  # plot point clouds
  color_list = []
 
  for line in pc_rgb:
    r = line[0]
    g = line[1]
    b = line[2]

    color = 'rgb({},{},{})'.format(r, g, b)
    color_list.append(color)

  fig = go.Figure(data=[go.Scatter3d(
    x=pc_xyz[:, 0],
    y=pc_xyz[:, 1],
    z=pc_xyz[:, 2],
    mode='markers',
    marker=dict(
        size=s,
        color=color_list,                # set color to an array/list of desired values
        colorscale='Viridis',   # choose a colorscale
        opacity=0.8
    ))])
  
  # plot cameras
  cam_wires_canonical = get_camera_wireframe().cuda()[None]
  cam_trans = cameras.get_world_to_view_transform().inverse()
  # cam_trans = cameras.get_world_to_view_transform()
  print(cam_wires_canonical)
  print()
  print(cam_trans)
  cam_wires_trans = cam_trans.transform_points(cam_wires_canonical)
  plot_handles = []
  for index, wire in enumerate(cam_wires_trans):
      x_, y_, z_ = wire.detach().cpu().numpy().T.astype(float)
      # # the Z and Y axes are flipped intentionally here!
      # x_, z_, y_ = wire.detach().cpu().numpy().T.astype(float)
      # (h,) = ax.plot(x_, y_, z_, color=color, linewidth=0.3)
      # plot_handles.append(h)
      fig.add_trace(go.Scatter3d(x=x_, 
                                 y=y_, 
                                 z=z_,
                                 mode='lines',
                                 name="Camera {}".format(index)))

  fig.show()


def plot_cameras(ax, cameras, color: str = "blue"):
    """
    Plots a set of `cameras` objects into the maplotlib axis `ax` with
    color `color`.
    """
    cam_wires_canonical = get_camera_wireframe().cuda()[None]

    cam_trans = cameras.get_world_to_view_transform().inverse()
    cam_wires_trans = cam_trans.transform_points(cam_wires_canonical)
    plot_handles = []
    for wire in cam_wires_trans:
        x_, y_, z_ = wire.detach().cpu().numpy().T.astype(float)
        # # the Z and Y axes are flipped intentionally here!
        # x_, z_, y_ = wire.detach().cpu().numpy().T.astype(float)
        (h,) = ax.plot(x_, y_, z_, color=color, linewidth=0.3)
        plot_handles.append(h)
    return plot_handles

def plot_camera_scene(ax, cameras, status: str):
    """
    Plots a set of predicted cameras `cameras` and their corresponding
    ground truth locations `cameras_gt`. The plot is named with
    a string passed inside the `status` argument.
    """

    ax.set_title(status)
    handle_cam = plot_cameras(ax, cameras, color="#FF7D1E")
    # plot_radius = 3
    # ax.set_xlim3d([-plot_radius, plot_radius])
    # ax.set_ylim3d([3 - plot_radius, 3 + plot_radius])
    # ax.set_zlim3d([-plot_radius, plot_radius])
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_zlabel("z")
    labels_handles = {
        "Cameras": handle_cam[0],
    }
    ax.legend(
        labels_handles.values(),
        labels_handles.keys(),
        loc="upper center",
        bbox_to_anchor=(0.5, 0),
    )
    return fig

plot_scene(cameras, pc_xyz, pc_rgb)

render_size_height = img_tensor.shape[1]
render_size_width = img_tensor.shape[2]
print(render_size_width, render_size_height)
volume_extent_world = 5 #bonding_box_size * 4

raysampler = NDCGridRaysampler(
    image_width=render_size_width,
    image_height=render_size_height,
    n_pts_per_ray=250,
    min_depth=0.1,
    max_depth=volume_extent_world,
)

raymarcher = EmissionAbsorptionRaymarcher()
renderer = VolumeRenderer(
    raysampler=raysampler, raymarcher=raymarcher,
)

class VolumeModel(torch.nn.Module):
    def __init__(self, renderer, volume_size=[64] * 3, voxel_size=0.1):
        super().__init__()
        # After evaluating torch.sigmoid(self.log_colors), we get
        # densities close to zero.
        self.log_densities = torch.nn.Parameter(-4.0 * torch.ones(1, *volume_size))
        # After evaluating torch.sigmoid(self.log_colors), we get
        # a neutral gray color everywhere.
        self.log_colors = torch.nn.Parameter(torch.zeros(3, *volume_size))
        self._voxel_size = voxel_size
        # Store the renderer module as well.
        self._renderer = renderer

    def forward(self, cameras):
        batch_size = cameras.R.shape[0]

        # Convert the log-space values to the densities/colors
        densities = torch.sigmoid(self.log_densities)
        colors = torch.sigmoid(self.log_colors)

        # Instantiate the Volumes object, making sure
        # the densities and colors are correctly
        # expanded batch_size-times.
        volumes = Volumes(
            densities=densities[None].expand(
                batch_size, *self.log_densities.shape),
            features=colors[None].expand(
                batch_size, *self.log_colors.shape),
            voxel_size=self._voxel_size,
        )

        # Given cameras and volumes, run the renderer
        # and return only the first output value
        # (the 2nd output is a representation of the sampled
        # rays which can be omitted for our purpose).
        return self._renderer(cameras=cameras, volumes=volumes)[0]


# A helper function for evaluating the smooth L1 (huber) loss
# between the rendered silhouettes and colors.
def huber(x, y, scaling=0.1):
    diff_sq = (x - y) ** 2
    loss = ((1 + diff_sq / (scaling ** 2)).clamp(1e-4).sqrt() - 1) * float(scaling)
    return loss

target_cameras = cameras.to(device)
target_images = img_tensor.to(device)

volume_size = 256
volume_model = VolumeModel(
    renderer,
    volume_size=[volume_size] * 3,
    voxel_size=volume_extent_world / volume_size,
).to(device)

# Instantiate the Adam optimizer. We set its master learning rate to 0.1.
lr = 0.05
optimizer = torch.optim.Adam(volume_model.parameters(), lr=lr)

# We do 300 Adam iterations and sample 10 random images in each minibatch.
batch_size = 5
n_iter = 750

for iteration in range(n_iter):
    if iteration == round(n_iter * 0.75):
        print('Decreasing LR 10-fold ...')
        optimizer = torch.optim.Adam(
            volume_model.parameters(), lr=lr * 0.1
        )

    # Zero the optimizer gradient.
    optimizer.zero_grad()

    # Sample random batch indices.
    batch_idx = torch.randperm(len(target_cameras))[:batch_size]

    # Sample the minibatch of cameras.
    batch_cameras = FoVPerspectiveCameras(
        R=target_cameras.R[batch_idx],
        T=target_cameras.T[batch_idx],
        znear=target_cameras.znear[batch_idx],
        zfar=target_cameras.zfar[batch_idx],
        aspect_ratio=target_cameras.aspect_ratio[batch_idx],
        fov=target_cameras.fov[batch_idx],
        device=device,
    )

    rendered_images, _ = volume_model(
        batch_cameras
    ).split([3, 1], dim=-1)

    color_err = huber(
        rendered_images, target_images[batch_idx],
    ).abs().mean()

    loss = color_err

    if iteration % 10 == 0:
        print(
            f'Iteration {iteration:05d}:'
            + f' color_err = {float(color_err):1.2e}'
        )

    # Take the optimization step.
    loss.backward()
    optimizer.step()

    # Visualize the renders every 40 iterations.
    if iteration % 40 == 0:
        # Visualize only a single randomly selected element of the batch.
        im_show_idx = int(torch.randint(low=0, high=batch_size, size=(1,)))
        fig, ax = plt.subplots(1, 2, figsize=(10, 10))
        ax = ax.ravel()
        clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()
        ax[0].imshow(clamp_and_detach(rendered_images[im_show_idx]))
        ax[1].imshow(clamp_and_detach(target_images[batch_idx[im_show_idx], ..., :3]))
        for ax_, title_ in zip(
                ax,
                ("rendered image", "target image")
        ):
            ax_.grid("off")
            ax_.axis("off")
            ax_.set_title(title_)
        fig.canvas.draw()
        fig.show()

def get_pts(infile):
    data = np.loadtxt(infile, skiprows=13, delimiter=' ')
    # return data[:, 0], data[:, 1], data[:, 2]  # returns X,Y,Z points skipping the first 12 lines
    return data[:, :3]  # returns X,Y,Z points skipping the first 12 lines


def plot_ply(ax, infile):
    # x, y, z = get_pts(infile)
    data = get_pts(infile)
    x, y, z = [data[:, i] for i in range(data.shape[1])]
    ax.scatter(x, y, z, c='r', marker='o')
    ax.set_xlabel('X Label')
    ax.set_ylabel('Y Label')
    ax.set_zlabel('Z Label')


def get_mean_point_cloud(xyz_ply):
    # find center of point cloud
    coordinates = np.mean(xyz_ply, axis=0)
    return coordinates.reshape(-1, 1)

def generate_shallow_rotating_volume(volume_model, camera_positions, lookat):
    distances = np.linalg.norm(lookat.T - camera_positions, axis=1).reshape(-1, 1)
    xs, ys, zs = camera_positions[:, 0].reshape(-1, 1), \
                 camera_positions[:, 1].reshape(-1, 1), \
                 camera_positions[:, 2].reshape(-1, 1)

    # centered xyz of cameras
    xsc = xs - lookat[0]
    ysc = ys - lookat[1]
    zsc = zs - lookat[2]

    elevs = np.arcsin(ysc / distances)
    azims = np.arctan2(xsc, zsc)

    distances = torch.tensor(distances).to(device).float()
    elevs = torch.tensor(elevs).to(device).float()
    azims = torch.tensor(azims).to(device).float()
    lookat = torch.tensor(lookat.T).to(device).float()

    Rs, Ts = look_at_view_transform(dist=distances, elev=elevs, azim=azims, at=lookat, degrees=False)

    frames = []

    for r, t in zip(Rs, Ts):
        camera = FoVPerspectiveCameras(
            R=r[None],
            T=t[None],
            device=device,
        )
        frames.append(volume_model(camera)[..., :3].clamp(0.0, 1.0))

    return torch.cat(frames)

def image_grid(
    images,
    rows=None,
    cols=None,
    fill: bool = True,
    show_axes: bool = False,
    rgb: bool = True,
):
    """
    A util function for plotting a grid of images.

    Args:
        images: (N, H, W, 4) array of RGBA images
        rows: number of rows in the grid
        cols: number of columns in the grid
        fill: boolean indicating if the space between images should be filled
        show_axes: boolean indicating if the axes of the plots should be visible
        rgb: boolean, If True, only RGB channels are plotted.
            If False, only the alpha channel is plotted.

    Returns:
        None
    """
    if (rows is None) != (cols is None):
        raise ValueError("Specify either both rows and cols or neither.")

    if rows is None:
        rows = len(images)
        cols = 1

    gridspec_kw = {"wspace": 0.0, "hspace": 0.0} if fill else {}
    fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(15, 9))
    bleed = 0
    fig.subplots_adjust(left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed))

    for ax, im in zip(axarr.ravel(), images):
        if rgb:
            # only render RGB channels
            ax.imshow(im[..., :3])
        else:
            # only render Alpha channel
            ax.imshow(im[..., 3])
        if not show_axes:
            ax.set_axis_off()

from scipy.linalg import lstsq
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
from pytorch3d.vis.plotly_vis import get_camera_wireframe

def align_z_axis(v):
    v = (v / np.linalg.norm(v)).reshape(3)
    x = v[0]
    y = v[1]
    z = v[2]

    # first rotate along z axis so that v is on yz plane
    # calculate alpha, which is the angle
    # from the projection of v to xy plane to positive y axis (0, 1, 0)
    p_v_xy = np.array([x, y])

    cos_alpha = np.dot(p_v_xy, np.array([0, 1])) / (np.linalg.norm(p_v_xy))
    if x >= 0:
        alpha = np.arccos(cos_alpha)
    else:
        alpha = np.pi * 2 - np.arccos(cos_alpha)

    R1 = np.array([[np.cos(alpha), -np.sin(alpha), 0],
                   [np.sin(alpha), np.cos(alpha), 0],
                   [0, 0, 1]])

    v1 = np.dot(R1, v)

    p_v_yz = np.array([v1[1], v1[2]])
    cos_beta = np.dot(p_v_yz, np.array([0, 1])) / (np.linalg.norm(p_v_yz))

    if y >= 0:
        beta = np.arccos(cos_beta)
    else:
        beta = np.pi * 2 - np.arccos(cos_beta)

    R2 = np.array([[1, 0, 0],
                   [0, np.cos(beta), -np.sin(beta)],
                   [0, np.sin(beta), np.cos(beta)]])

    R = np.dot(R2, R1)
    return R

from scipy.optimize import curve_fit
def func(_3d_points, a, b, c):
  x = _3d_points[:, 0]
  y = _3d_points[:, 1]
  return a * x + b * y + c


def generate_new_cameras_curvfitting(camera_positions):
  camera_positions = camera_positions.reshape(-1, 3)
  num_cameras = camera_positions.shape[0]
  x = camera_positions[:, 0]
  y = camera_positions[:, 1]
  z = camera_positions[:, 2]
  
  min_x = np.min(x)
  max_x = np.max(x)
  min_y = np.min(y)
  max_y = np.max(y)

  # xs = np.linspace(min_x, max_x, num_cameras, endpoint=True).reshape(-1, 1)
  # ys = np.linspace(min_y, max_y, num_cameras, endpoint=True).reshape(-1, 1)
  xs = camera_positions[:, 0].reshape(-1, 1)
  ys = camera_positions[:, 1].reshape(-1, 1)

  popt, _ = curve_fit(func, camera_positions[:, :2], camera_positions[:, 2])

  zs = func(np.hstack([xs, ys]), *popt).reshape(-1, 1)

  new_camera_positions = np.hstack([xs, ys, zs])

  plt.figure()
  ax = plt.subplot(111, projection='3d')
  ax.scatter(camera_positions[:, 0], camera_positions[:, 1], camera_positions[:, 2], color='g')
  ax.scatter(new_camera_positions[:, 0], new_camera_positions[:, 1], new_camera_positions[:, 2], color='b')
  ax.set_xlabel('x')
  ax.set_ylabel('y')
  ax.set_zlabel('z')
  plt.show()
  
  return new_camera_positions


def generate_new_cameras(camera_positions, lookat, num_cameras=4, 
                         visualize=False,
                         enclosing_radius_coef=0.1):
    camera_positions = camera_positions.reshape(-1, 3)
    camera_center = np.mean(camera_positions, axis=0)
    cameras_enclosing_radius = enclosing_radius_coef * np.mean(np.linalg.norm(camera_positions - camera_center.reshape(1, 3), axis=1))
    camera_plane_distance = np.linalg.norm(camera_center - lookat.reshape(-1, 3))

    A = camera_positions[:, :2]
    A = np.c_[A, np.ones(A.shape[0]).reshape(-1, 1)]
    b = camera_positions[:, 2].reshape(-1, 1)
    fit, residual, rnk, s = lstsq(A, b)
    print("fitted residual is: ", residual)
    if visualize:
        plt.figure()
        ax = plt.subplot(111, projection='3d')
        ax.scatter(camera_positions[:, 0], camera_positions[:, 1], camera_positions[:, 2], color='g')
        # plot plane
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()
        X, Y = np.meshgrid(np.arange(xlim[0], xlim[1]),
                           np.arange(ylim[0], ylim[1]))
        Z = np.zeros(X.shape)
        for r in range(X.shape[0]):
            for c in range(X.shape[1]):
                Z[r, c] = fit[0] * X[r, c] + fit[1] * Y[r, c] + fit[2]
        # ax.plot_wireframe(X, Y, Z, color='k')
        # plot center of ply
        ax.scatter(lookat[0], lookat[1], lookat[2], c='orange')

    fit = fit.ravel()
    popt = np.array([fit[0], fit[1], 1, fit[2]]).ravel()    # the fitted function is ax + by + c = z => popt = (a, b, 1, c)
    normal = np.array([popt[0], popt[1], popt[2]])

    ##############
    normal = (lookat.reshape(-1, 3) - camera_center).ravel()

    ax.plot([0, normal[0]], [0, normal[1]], [0, normal[2]])

    normal = normal / np.linalg.norm(normal)
    print("normal is: ", normal)
    # check normal direction
    cos_angle = np.dot(normal.reshape(1, 3), lookat.reshape(3, 1) - camera_center.reshape(3, 1))
    if cos_angle < 0:
        normal = -normal

    R = align_z_axis(normal)
    print("Rotated: ", np.dot(R, normal.reshape(-1, 1)))

    # generate camera positions in a circle on xy plane
    angles_to_xaxis = np.linspace(0, np.pi * 2, num_cameras, endpoint=False)
    xs = np.cos(angles_to_xaxis) * cameras_enclosing_radius
    ys = np.sin(angles_to_xaxis) * cameras_enclosing_radius
    new_camera_positions = np.hstack([xs.reshape(-1, 1), ys.reshape(-1, 1), np.zeros((len(xs), 1))])
    new_camera_positions = np.dot(R.T, new_camera_positions.T).T + camera_center.reshape(1, 3)
    #
    if visualize:
        ax.scatter(new_camera_positions[:, 0], new_camera_positions[:, 1], new_camera_positions[:, 2], color='b')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_zlabel('z')
        plt.show()
    return new_camera_positions

with torch.no_grad():
  camera_positions = cameras.get_camera_center().cpu().numpy()

  camera_positions_new = generate_new_cameras_curvfitting(camera_positions)

with torch.no_grad():
    # rotating_volume_frames = generate_rotating_volume(volume_model, n_frames=7 * 4)
    ply_file = root_dir + '/pointclouds/test1.ply'
    xyz_ply = get_pts(ply_file)
    mean_ply = get_mean_point_cloud(xyz_ply)
    # mean_ply = np.array([2, 2, 15]).reshape(-1, 1)
    print(mean_ply)
    camera_positions = cameras.get_camera_center().cpu().numpy()
    new_camera_positions = generate_new_cameras(camera_positions, mean_ply,
                                                camera_positions.shape[0], visualize=True)  # this is the center
    rotating_volume_frames = generate_shallow_rotating_volume(volume_model, new_camera_positions, mean_ply)

image_grid(rotating_volume_frames.clamp(0., 1.).cpu().numpy(), rows=4, cols=7, rgb=True, fill=True)
plt.show()

torch.cuda.empty_cache()

